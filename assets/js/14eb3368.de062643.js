(self.webpackChunk=self.webpackChunk||[]).push([[817],{2015:(e,t,a)=>{var n={"./docs/configuration/contract.md":3103,"./docs/configuration/lakeformation.md":919,"./docs/configuration/materializations/hive-ha.md":4372,"./docs/configuration/materializations/hive.md":9481,"./docs/configuration/materializations/iceberg.md":9136,"./docs/configuration/seeds.md":3836,"./docs/configuration/snapshots.md":9081,"./docs/configuration/table-configuration.md":835,"./docs/contributing/contributing.md":9482,"./docs/contributing/local-development.md":9697,"./docs/getting-started/installation.md":9641,"./docs/getting-started/prerequisites/aws-resources.md":4047,"./docs/getting-started/prerequisites/iam-permissions.md":4632,"./docs/introduction.md":2057,"./docs/known-issues.md":7526,"./docs/migration/UPGRADE-1.5.md":7368,"./faqs/Athena/dbt-threads.md":9252,"./faqs/Athena/too-many-open-partitions.md":9109};function i(e){var t=o(e);return a(t)}function o(e){if(!a.o(n,e)){var t=new Error("Cannot find module '"+e+"'");throw t.code="MODULE_NOT_FOUND",t}return n[e]}i.keys=function(){return Object.keys(n)},i.resolve=o,e.exports=i,i.id=2015},2691:(e,t,a)=>{"use strict";a.d(t,{Z:()=>h});var n=a(7294),i=a(6010),o=a(9960),r=a(3791),s=a(3919),l=a(5999);const d={cardContainer:"cardContainer_S8oU",cardTitle:"cardTitle_HoSo",cardDescription:"cardDescription_c27F",glossaryCard:"glossaryCard_nmCw"};function p(e){var t=e.href,a=e.children;return n.createElement(o.Z,{href:t,className:(0,i.Z)("card padding--lg",d.cardContainer,t.includes("/terms/")&&d.glossaryCard)},a)}function c(e){var t=e.href,a=e.icon,o=e.title,r=e.description,s=e.hoverSnippet;return n.createElement(p,{href:t},n.createElement("h2",{className:(0,i.Z)(!t.includes("/terms/")&&"text--truncate",d.cardTitle),title:o},a," ",o),r&&n.createElement("p",{className:(0,i.Z)(!t.includes("/terms/")&&"text--truncate",d.cardDescription),title:s||r},s||r))}function m(e){var t=e.item,a=(0,r.Wl)(t);return a?n.createElement(c,{href:a,icon:"\ud83d\uddc3\ufe0f",title:t.label,description:(0,l.I)({message:"{count} items",id:"theme.docs.DocCard.categoryDescription",description:"The default description for a category card in the generated index about how many items this category includes"},{count:t.items.length})}):null}function u(e){var t,i,o=e.item,l=(0,s.Z)(o.href)?"\ud83d\udcc4\ufe0f":"\ud83d\udd17",d=(0,r.xz)(null!=(t=o.docId)?t:void 0);if(o.docId&&o.href&&o.href.includes("/terms/")){var p=a(2015)("./"+o.docId+".md");p&&(i=p.frontMatter.hoverSnippet)}return n.createElement(c,{href:o.href,icon:l,title:o.label,description:null==d?void 0:d.description,hoverSnippet:i})}function h(e){var t=e.item;switch(t.type){case"link":return n.createElement(u,{item:t});case"category":return n.createElement(m,{item:t});default:throw new Error("unknown item type "+JSON.stringify(t))}}},3103:(e,t,a)=>{"use strict";a.r(t),a.d(t,{assets:()=>p,contentTitle:()=>l,default:()=>h,frontMatter:()=>s,metadata:()=>d,toc:()=>c});var n=a(7462),i=a(3366),o=(a(7294),a(3905)),r=["components"],s={title:"Contract & Constraints",id:"contract-constraints"},l=void 0,d={unversionedId:"docs/configuration/contract-constraints",id:"docs/configuration/contract-constraints",title:"Contract & Constraints",description:"Contract",source:"@site/docs/docs/configuration/contract.md",sourceDirName:"docs/configuration",slug:"/docs/configuration/contract-constraints",permalink:"/docs/configuration/contract-constraints",draft:!1,editUrl:"https://github.com/dbt-athena/dbt-athena.github.io/edit/main/docs/docs/configuration/contract.md",tags:[],version:"current",lastUpdatedAt:1706174898,formattedLastUpdatedAt:"Jan 25, 2024",frontMatter:{title:"Contract & Constraints",id:"contract-constraints"},sidebar:"docs",previous:{title:"Snapshots",permalink:"/docs/configuration/snapshots"},next:{title:"Lakeformation",permalink:"/docs/configuration/lakeformation"}},p={},c=[{value:"Contract",id:"contract",level:2},{value:"Constraints",id:"constraints",level:2}],m={toc:c},u="wrapper";function h(e){var t=e.components,a=(0,i.Z)(e,r);return(0,o.kt)(u,(0,n.Z)({},m,a,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h2",{id:"contract"},"Contract"),(0,o.kt)("p",null,"The adapter partly supports ",(0,o.kt)("a",{parentName:"p",href:"https://docs.getdbt.com/reference/resource-configs/contract"},"contract")," definition."),(0,o.kt)("p",null,"Concerning the ",(0,o.kt)("inlineCode",{parentName:"p"},"data_type"),", it is supported but needs to be adjusted for complex types. They must be specified\nentirely (for instance ",(0,o.kt)("inlineCode",{parentName:"p"},"array<int>"),") even though they won't be checked. Indeed, as dbt recommends, we only compare\nthe broader type (array, map, int, varchar). The complete definition is used in order to check that the data types\ndefined in athena are ok (pre-flight check)."),(0,o.kt)("h2",{id:"constraints"},"Constraints"),(0,o.kt)("p",null,"The adapter does not support ",(0,o.kt)("a",{parentName:"p",href:"https://docs.getdbt.com/reference/resource-properties/constraints"},"constraints")," since\nconstraints don't exist in Athena."))}h.isMDXComponent=!0},919:(e,t,a)=>{"use strict";a.r(t),a.d(t,{assets:()=>p,contentTitle:()=>l,default:()=>h,frontMatter:()=>s,metadata:()=>d,toc:()=>c});var n=a(7462),i=a(3366),o=(a(7294),a(3905)),r=["components"],s={title:"Lakeformation",id:"lakeformation"},l=void 0,d={unversionedId:"docs/configuration/lakeformation",id:"docs/configuration/lakeformation",title:"Lakeformation",description:"Tags",source:"@site/docs/docs/configuration/lakeformation.md",sourceDirName:"docs/configuration",slug:"/docs/configuration/lakeformation",permalink:"/docs/configuration/lakeformation",draft:!1,editUrl:"https://github.com/dbt-athena/dbt-athena.github.io/edit/main/docs/docs/configuration/lakeformation.md",tags:[],version:"current",lastUpdatedAt:1706174898,formattedLastUpdatedAt:"Jan 25, 2024",frontMatter:{title:"Lakeformation",id:"lakeformation"},sidebar:"docs",previous:{title:"Contract & Constraints",permalink:"/docs/configuration/contract-constraints"},next:{title:"Known issues",permalink:"/docs/known-issues"}},p={},c=[{value:"Tags",id:"tags",level:2},{value:"Data Cell Filters",id:"data-cell-filters",level:2}],m={toc:c},u="wrapper";function h(e){var t=e.components,a=(0,i.Z)(e,r);return(0,o.kt)(u,(0,n.Z)({},m,a,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h2",{id:"tags"},"Tags"),(0,o.kt)("p",null,"The adapter implements AWS Lakeformation tags management in the following way:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"you can enable or disable lf-tags management via ",(0,o.kt)("a",{parentName:"li",href:"./table-configuration"},"config")," (disabled by default).\nHere are config examples:")),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},"model_config.sql"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sql"},"{{\n  config(\n    materialized='incremental',\n    incremental_strategy='append',\n    on_schema_change='append_new_columns',\n    table_type='iceberg',\n    schema='test_schema',\n    lf_tags_config={\n          'enabled': true,\n          'tags': {\n            'tag1': 'value1',\n            'tag2': 'value2'\n          },\n          'tags_columns': {\n            'tag1': {\n              'value1': ['column1', 'column2'],\n              'value2': ['column3', 'column4']\n            }\n          }\n    }\n  )\n}}\n")),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},"dbt_project.yml"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},"  +lf_tags_config:\n    enabled: true\n    tags:\n      tag1: value1\n      tag2: value2\n    tags_columns:\n      tag1:\n        value1: [ column1, column2 ]\n")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"once you enable the feature, lf-tags will be updated on every dbt run"),(0,o.kt)("li",{parentName:"ul"},"first, all lf-tags for ",(0,o.kt)("strong",{parentName:"li"},"columns")," are removed to avoid inheritance issues"),(0,o.kt)("li",{parentName:"ul"},"then all redundant lf-tags are removed from ",(0,o.kt)("strong",{parentName:"li"},"table")," and actual tags from config are applied"),(0,o.kt)("li",{parentName:"ul"},"finally, lf-tags for ",(0,o.kt)("strong",{parentName:"li"},"columns")," are applied")),(0,o.kt)("admonition",{type:"info"},(0,o.kt)("p",{parentName:"admonition"},"It's important to understand the following points:"),(0,o.kt)("ul",{parentName:"admonition"},(0,o.kt)("li",{parentName:"ul"},"dbt does not manage lf-tags for database"),(0,o.kt)("li",{parentName:"ul"},"dbt does not manage lakeformation permissions")),(0,o.kt)("p",{parentName:"admonition"},"That's why you should handle this by yourself manually or using some automation tools like terraform, AWS CDK etc.",(0,o.kt)("br",{parentName:"p"}),"\n","You may find the following links useful to manage that:"),(0,o.kt)("ul",{parentName:"admonition"},(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/lakeformation_permissions"},"terraform aws_lakeformation_permissions")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/lakeformation_resource_lf_tags"},"terraform aws_lakeformation_resource_lf_tags")))),(0,o.kt)("h2",{id:"data-cell-filters"},"Data Cell Filters"),(0,o.kt)("p",null,"The adapter implements AWS Lakeformation Data Cell Filters management in the following way.",(0,o.kt)("br",{parentName:"p"}),"\n",(0,o.kt)("inlineCode",{parentName:"p"},"model_config.sql"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sql"},"{{\n    config(\n    materialized='incremental',\n    incremental_strategy='append',\n    on_schema_change='append_new_columns',\n    table_type='iceberg',\n    schema='test_schema',\n    lf_grants={\n          'data_cell_filters': {\n              'enabled': True | False,\n              'filters': {\n                  'filter_name': {\n                      'row_filter': '<filter_condition>',\n                      'principals': ['principal_arn1', 'principal_arn2']\n                  }\n              }\n          }\n      }\n}}\n")),(0,o.kt)("p",null,"or more advanced example for ",(0,o.kt)("inlineCode",{parentName:"p"},"dbt_project.yml")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},'models:\n    directory:\n      +schema: your_schema\n      +materialized: incremental\n      +on_schema_change: sync_all_columns\n      model1:\n        +lf_grants: &default_rls\n          data_cell_filters:\n            enabled: true\n            filters:\n              name1:\n                row_filter: "field1 = \'value1\'"\n                principals:\n                  - "role1_arn"\n                  - "role2_arn"\n              name2:\n                row_filter: "field1 = \'value2\'"\n                principals:\n                  - "role3_arn"\n                  - "role4_arn"\n      model2:\n        +lf_grants: *default_rls  # reuse previously defined config\n')),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Data cell filters management can't be automated outside dbt because the filter can't be attached to the table\nwhich doesn't exist.  "),(0,o.kt)("li",{parentName:"ul"},"Once you ",(0,o.kt)("inlineCode",{parentName:"li"},"enable")," this config, dbt will set all filters and their permissions during every dbt run.  "),(0,o.kt)("li",{parentName:"ul"},"Such approach keeps the actual state of row level security configuration actual after every dbt run and\napplies changes if they occur: drop, create, update filters and their permissions.")),(0,o.kt)("admonition",{type:"caution"},(0,o.kt)("p",{parentName:"admonition"},"It's important to understand that LF permissions work like ",(0,o.kt)("inlineCode",{parentName:"p"},"union"),".",(0,o.kt)("br",{parentName:"p"}),"\n","Let's imagine this scenario:"),(0,o.kt)("ul",{parentName:"admonition"},(0,o.kt)("li",{parentName:"ul"},"Table X has tag ",(0,o.kt)("inlineCode",{parentName:"li"},"domain=foo")),(0,o.kt)("li",{parentName:"ul"},"Role A has ",(0,o.kt)("inlineCode",{parentName:"li"},"select")," permission for tables with ",(0,o.kt)("inlineCode",{parentName:"li"},"domain=foo")),(0,o.kt)("li",{parentName:"ul"},"We add a data cell filter for a column in table X and then grant permissions to role A")),(0,o.kt)("p",{parentName:"admonition"},"In this case, tag permissions are the ones considered, and cell-level permissions are totally ignored.\nThis means that this data cell filters management feature implies that you should use permissions for specific tables\nwhich don't have already tag-level permissions in that specific database or table.")))}h.isMDXComponent=!0},4372:(e,t,a)=>{"use strict";a.r(t),a.d(t,{assets:()=>m,contentTitle:()=>p,default:()=>b,frontMatter:()=>d,metadata:()=>c,toc:()=>u});var n,i=a(7462),o=a(3366),r=(a(7294),a(3905)),s=a(5108),l=["components"],d={title:"Highly-available Hive table",id:"hive-ha"},p=void 0,c={unversionedId:"docs/configuration/materializations/hive-ha",id:"docs/configuration/materializations/hive-ha",title:"Highly-available Hive table",description:"This feature changed in version 1.5.",source:"@site/docs/docs/configuration/materializations/hive-ha.md",sourceDirName:"docs/configuration/materializations",slug:"/docs/configuration/materializations/hive-ha",permalink:"/docs/configuration/materializations/hive-ha",draft:!1,editUrl:"https://github.com/dbt-athena/dbt-athena.github.io/edit/main/docs/docs/configuration/materializations/hive-ha.md",tags:[],version:"current",lastUpdatedAt:1706174898,formattedLastUpdatedAt:"Jan 25, 2024",frontMatter:{title:"Highly-available Hive table",id:"hive-ha"},sidebar:"docs",previous:{title:"Hive table",permalink:"/docs/configuration/materializations/hive"},next:{title:"Iceberg table",permalink:"/docs/configuration/materializations/iceberg"}},m={},u=[{value:"Known issues",id:"known-issues",level:4}],h=(n="VersionBlock",function(e){return s.warn("Component "+n+" was not imported, exported, or provided by MDXProvider as global scope"),(0,r.kt)("div",e)}),g={toc:u},k="wrapper";function b(e){var t=e.components,a=(0,o.Z)(e,l);return(0,r.kt)(k,(0,i.Z)({},g,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)(h,{firstVersion:"1.5",mdxType:"VersionBlock"},(0,r.kt)("admonition",{type:"caution"},(0,r.kt)("p",{parentName:"admonition"},"This feature changed in version 1.5.\nVisit the ",(0,r.kt)("a",{parentName:"p",href:"/docs/configuration/materializations/hive"},"hive materialization")," page for more information, or browse the\n",(0,r.kt)("a",{parentName:"p",href:"/docs/migration/UPGRADE-1.5"},"migration guide")," for more details."))),(0,r.kt)("p",null,"The current implementation of the Hive table materialization can lead to downtime, because the target table is dropped and re-created. To have a less destructive behavior, it's possible to use the ",(0,r.kt)("inlineCode",{parentName:"p"},"table='table_hive_ha'")," materialization."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("inlineCode",{parentName:"strong"},"table_hive_ha"))," leverages the ",(0,r.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/glue/latest/webapi/API_GetTableVersions.html"},"table versions"),' feature of Glue catalog: creating a "tmp" table and swapping the target table to the location of the tmp table.'),(0,r.kt)("admonition",{type:"info"},(0,r.kt)("p",{parentName:"admonition"},"This materialization is only available for ",(0,r.kt)("inlineCode",{parentName:"p"},"table_type=hive")," (default) and requires using unique locations on S3.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"{{\n  config(\n    materialized='table_hive_ha',\n    format='parquet',\n    partitioned_by=['status'],\n    s3_data_naming='table_unique'\n  )\n}}\n\nselect\n    'a' as user_id,\n    'pi' as user_name,\n    'active' as status\nunion all\nselect\n    'b' as user_id,\n    'sh' as user_name,\n    'active' as status\nunion all\nselect\n    'c' as user_id,\n    'sh' as user_name,\n    'disabled' as status\n")),(0,r.kt)("p",null,"By default, the materialization keeps the last 4 table versions. You can change it by setting ",(0,r.kt)("inlineCode",{parentName:"p"},"versions_to_keep")," on the model configuration."),(0,r.kt)("h4",{id:"known-issues"},"Known issues"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"When swapping from a table with partitions to a table without (and the other way around), there could be a little downtime. In case high performance is needed, consider bucketing instead of partitions"),(0,r.kt)("li",{parentName:"ul"},'By default, Glue "duplicate" the versions internally, so the last 2 versions of a table point to the same location. Therefore it\'s recommended to use ',(0,r.kt)("inlineCode",{parentName:"li"},"versions_to_keep")," >= 4, as this will avoid to have the older location removed.")))}b.isMDXComponent=!0},9481:(e,t,a)=>{"use strict";a.r(t),a.d(t,{assets:()=>m,contentTitle:()=>p,default:()=>b,frontMatter:()=>d,metadata:()=>c,toc:()=>u});var n,i=a(7462),o=a(3366),r=(a(7294),a(3905)),s=a(5108),l=["components"],d={title:"Hive table",id:"hive"},p="Hive",c={unversionedId:"docs/configuration/materializations/hive",id:"docs/configuration/materializations/hive",title:"Hive table",description:"The default dbt table materialization will be using a Hive",source:"@site/docs/docs/configuration/materializations/hive.md",sourceDirName:"docs/configuration/materializations",slug:"/docs/configuration/materializations/hive",permalink:"/docs/configuration/materializations/hive",draft:!1,editUrl:"https://github.com/dbt-athena/dbt-athena.github.io/edit/main/docs/docs/configuration/materializations/hive.md",tags:[],version:"current",lastUpdatedAt:1706174898,formattedLastUpdatedAt:"Jan 25, 2024",frontMatter:{title:"Hive table",id:"hive"},sidebar:"docs",previous:{title:"Table configuration",permalink:"/docs/configuration/table-configuration"},next:{title:"Highly-available Hive table",permalink:"/docs/configuration/materializations/hive-ha"}},m={},u=[{value:"Reducing downtime",id:"reducing-downtime",level:2},{value:"Known issues",id:"known-issues",level:4}],h=(n="VersionBlock",function(e){return s.warn("Component "+n+" was not imported, exported, or provided by MDXProvider as global scope"),(0,r.kt)("div",e)}),g={toc:u},k="wrapper";function b(e){var t=e.components,a=(0,o.Z)(e,l);return(0,r.kt)(k,(0,i.Z)({},g,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"hive"},"Hive"),(0,r.kt)("p",null,"The default dbt ",(0,r.kt)("a",{parentName:"p",href:"https://docs.getdbt.com/docs/build/materializations#table"},"table materialization")," will be using a Hive\ntable in Athena.\nWhen used, dbt will run a ",(0,r.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/athena/latest/ug/create-table-as.html"},(0,r.kt)("inlineCode",{parentName:"a"},"CREATE TABLE AS"))," query\nagainst Athena."),(0,r.kt)(h,{firstVersion:"1.5",mdxType:"VersionBlock"},(0,r.kt)("h2",{id:"reducing-downtime"},"Reducing downtime"),(0,r.kt)("p",null,"The current implementation of the Hive table materialization can lead to downtime, because the target table is\ndropped and re-created. To have a less destructive behavior, it's possible to use the ",(0,r.kt)("inlineCode",{parentName:"p"},"ha=true")," config in your model\nconfiguration (disabled by default). We had a specific materialization for that, called ",(0,r.kt)("inlineCode",{parentName:"p"},"table_hive_ha"),", which was\nmerged in the ",(0,r.kt)("inlineCode",{parentName:"p"},"table")," materialization."),(0,r.kt)("p",null,"The ha config leverages the ",(0,r.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/glue/latest/webapi/API_GetTableVersions.html"},"table versions"),'\nfeature of Glue catalog: creating a "tmp" table and swapping the target table to the location of the tmp table.'),(0,r.kt)("admonition",{type:"info"},(0,r.kt)("p",{parentName:"admonition"},"This materialization is only available for ",(0,r.kt)("inlineCode",{parentName:"p"},"table_type=hive")," (default) and requires using unique locations on S3.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"{{\n  config(\n    materialized='table',\n    ha=true,\n    format='parquet',\n    partitioned_by=['status'],\n    s3_data_naming='table_unique'\n  )\n}}\n\nselect\n    'a' as user_id,\n    'pi' as user_name,\n    'active' as status\nunion all\nselect\n    'b' as user_id,\n    'sh' as user_name,\n    'active' as status\nunion all\nselect\n    'c' as user_id,\n    'sh' as user_name,\n    'disabled' as status\n")),(0,r.kt)("p",null,"By default, only the last 4 table versions are kept. You can change it by setting ",(0,r.kt)("inlineCode",{parentName:"p"},"versions_to_keep")," on the model\nconfiguration."),(0,r.kt)("h4",{id:"known-issues"},"Known issues"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"When swapping from a table with partitions to a table without (and the other way around), there could be a little\ndowntime. In case high performance is needed, consider bucketing instead of partitions"),(0,r.kt)("li",{parentName:"ul"},'By default, Glue "duplicates" the versions internally, so the last 2 versions of a table point to the same location.\nTherefore it\'s recommended to use ',(0,r.kt)("inlineCode",{parentName:"li"},"versions_to_keep")," >= 4, as this will avoid to have the older location removed."))))}b.isMDXComponent=!0},9136:(e,t,a)=>{"use strict";a.r(t),a.d(t,{assets:()=>m,contentTitle:()=>p,default:()=>b,frontMatter:()=>d,metadata:()=>c,toc:()=>u});var n,i=a(7462),o=a(3366),r=(a(7294),a(3905)),s=a(5108),l=["components"],d={title:"Iceberg table",id:"iceberg"},p="Apache Iceberg",c={unversionedId:"docs/configuration/materializations/iceberg",id:"docs/configuration/materializations/iceberg",title:"Iceberg table",description:"Athena supports read, time travel, write, and DDL queries for Apache Iceberg tables that use the Apache Parquet format",source:"@site/docs/docs/configuration/materializations/iceberg.md",sourceDirName:"docs/configuration/materializations",slug:"/docs/configuration/materializations/iceberg",permalink:"/docs/configuration/materializations/iceberg",draft:!1,editUrl:"https://github.com/dbt-athena/dbt-athena.github.io/edit/main/docs/docs/configuration/materializations/iceberg.md",tags:[],version:"current",lastUpdatedAt:1706174898,formattedLastUpdatedAt:"Jan 25, 2024",frontMatter:{title:"Iceberg table",id:"iceberg"},sidebar:"docs",previous:{title:"Highly-available Hive table",permalink:"/docs/configuration/materializations/hive-ha"},next:{title:"Seeds",permalink:"/docs/configuration/seeds"}},m={},u=[{value:"Getting started",id:"getting-started",level:2},{value:"Incremental tables",id:"incremental-tables",level:2}],h=(n="VersionBlock",function(e){return s.warn("Component "+n+" was not imported, exported, or provided by MDXProvider as global scope"),(0,r.kt)("div",e)}),g={toc:u},k="wrapper";function b(e){var t=e.components,a=(0,o.Z)(e,l);return(0,r.kt)(k,(0,i.Z)({},g,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"apache-iceberg"},"Apache Iceberg"),(0,r.kt)("p",null,"Athena supports read, time travel, write, and DDL queries for Apache Iceberg tables that use the Apache Parquet format\nfor data and the AWS Glue catalog for their metastore."),(0,r.kt)("p",null,"Apache Iceberg is an open table format for huge analytic datasets.\nIceberg manages large collections of files as tables, and it supports modern analytical data lake operations such as\nrecord-level insert, update, delete, and time travel queries.\nThe Iceberg specification allows seamless table evolution such as schema and partition evolution and is designed for\noptimized usage on Amazon S3.\nIceberg also helps guarantee data correctness under concurrent write scenarios."),(0,r.kt)("p",null,"The dbt-athena adapter supports table materialization for Apache Iceberg."),(0,r.kt)("h2",{id:"getting-started"},"Getting started"),(0,r.kt)("p",null,"To get started, add the following config block to your model:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"{{\n  config(\n    materialized='table',\n    table_type='iceberg',\n    format='parquet',\n    partitioned_by=['bucket(user_id, 5)'],\n    table_properties={\n      'optimize_rewrite_delete_file_threshold': '2'\n    }\n  )\n}}\n\nSELECT 'A'          AS user_id,\n       'pi'         AS name,\n       'active'     AS status,\n       17.89        AS cost,\n       1            AS quantity,\n       100000000    AS quantity_big,\n       current_date AS my_date\n")),(0,r.kt)("p",null,"Iceberg supports ",(0,r.kt)("strong",{parentName:"p"},"bucketing")," as hidden partitions, therefore, use the ",(0,r.kt)("inlineCode",{parentName:"p"},"partitioned_by")," config to add specific\nbucketing conditions."),(0,r.kt)("p",null,"Iceberg supports several ",(0,r.kt)("strong",{parentName:"p"},"table formats")," for data : ",(0,r.kt)("inlineCode",{parentName:"p"},"PARQUET"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"AVRO")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"ORC"),"."),(0,r.kt)("h2",{id:"incremental-tables"},"Incremental tables"),(0,r.kt)("p",null,"It is possible to use iceberg in an incremental materialization. Two strategies are supported:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"append"),": New records are appended to the table, this can lead to duplicates."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"merge"),": Performs an upsert (and optionally delete), where new records are added and existing records are updated.\nOnly available with Athena engine version 3.")),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"merge")," configuration"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"unique_key")," ",(0,r.kt)("strong",{parentName:"li"},"(required)"),": columns that define a unique record in the source and target tables.")),(0,r.kt)(h,{firstVersion:"1.5.1",mdxType:"VersionBlock"},(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"incremental_predicates")," (optional): SQL conditions that enable custom join clauses in the merge statement. This can\nbe useful for improving performance via predicate pushdown on the target table.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"delete_condition")," (optional): SQL condition used to identify records that should be deleted."))),(0,r.kt)(h,{firstVersion:"1.6.2",mdxType:"VersionBlock"},(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"update_condition")," (optional): SQL condition used to identify records that should be updated."))),(0,r.kt)(h,{firstVersion:"1.6.4",mdxType:"VersionBlock"},(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"insert_condition")," (optional): SQL condition used to identify records that should be inserted."))),(0,r.kt)("p",null,"These configurations can include any column of the incremental table (",(0,r.kt)("inlineCode",{parentName:"p"},"src"),") or the final table (",(0,r.kt)("inlineCode",{parentName:"p"},"target"),").\nColumn names must be prefixed by either ",(0,r.kt)("inlineCode",{parentName:"p"},"src")," or ",(0,r.kt)("inlineCode",{parentName:"p"},"target")," to prevent a ",(0,r.kt)("inlineCode",{parentName:"p"},"Column is ambiguous")," error.")),(0,r.kt)(h,{firstVersion:"1.4.4",mdxType:"VersionBlock"},(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"delete_condition")," (optional): SQL condition used to identify records that should be deleted. Can include any column\nof the incremental table (",(0,r.kt)("inlineCode",{parentName:"li"},"src"),") or the final table (",(0,r.kt)("inlineCode",{parentName:"li"},"target"),"). Column names must be prefixed by either ",(0,r.kt)("inlineCode",{parentName:"li"},"src"),"\nor ",(0,r.kt)("inlineCode",{parentName:"li"},"target")," to prevent a ",(0,r.kt)("inlineCode",{parentName:"li"},"Column is ambiguous")," error."))),(0,r.kt)("admonition",{type:"caution"},(0,r.kt)("p",{parentName:"admonition"},(0,r.kt)("inlineCode",{parentName:"p"},"MERGE INTO")," is transactional and is supported only for Apache Iceberg tables in ",(0,r.kt)("strong",{parentName:"p"},"Athena engine version 3"))),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"incremental_predicates & delete_condition")," example:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"{{ config(\n    materialized='incremental',\n    table_type='iceberg',\n    incremental_strategy='merge',\n    unique_key='user_id',\n    incremental_predicates=[\"src.quantity > 1\", \"target.my_date >= now() - interval '4' year\"],\n    delete_condition=\"src.status != 'active' and target.my_date < now() - interval '2' year\",\n    format='parquet'\n) }}\n\nselect 'A'          as user_id,\n       'pi'         as name,\n       'active'     as status,\n       17.89        as cost,\n       1            as quantity,\n       100000000    as quantity_big,\n       current_date as my_date\n")),(0,r.kt)(h,{firstVersion:"1.6.2",mdxType:"VersionBlock"},(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"update_condition")," example:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"{{ config(\n        materialized='incremental',\n        incremental_strategy='merge',\n        unique_key=['id'],\n        update_condition='target.id > 1',\n        schema='sandbox'\n    )\n}}\n\n{% if is_incremental() %}\n\nselect * from (\n    values\n    (1, 'v1-updated')\n    , (2, 'v2-updated')\n) as t (id, value)\n\n{% else %}\n\nselect * from (\n    values\n    (-1, 'v-1')\n    , (0, 'v0')\n    , (1, 'v1')\n    , (2, 'v2')\n) as t (id, value)\n\n{% endif %}\n"))),(0,r.kt)(h,{firstVersion:"1.6.4",mdxType:"VersionBlock"},(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"insert_condition")," example:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"{{ config(\n        table_type='iceberg',\n        materialized='incremental',\n        incremental_strategy='merge',\n        unique_key=['id'],\n        insert_condition='src.status != 0'\n    )\n}}\n\n{% if is_incremental() %}\n\nselect * from (\n    values\n    (1, -1)\n    , (2, 0)\n    , (3, 1)\n) as t (id, status)\n\n{% else %}\n\nselect * from (\n    values\n    (0, 1)\n) as t (id, status)\n\n{% endif %}\n"))))}b.isMDXComponent=!0},3836:(e,t,a)=>{"use strict";a.r(t),a.d(t,{assets:()=>p,contentTitle:()=>l,default:()=>h,frontMatter:()=>s,metadata:()=>d,toc:()=>c});var n=a(7462),i=a(3366),o=(a(7294),a(3905)),r=["components"],s={title:"Seeds",id:"seeds"},l=void 0,d={unversionedId:"docs/configuration/seeds",id:"docs/configuration/seeds",title:"Seeds",description:"Seeds are CSV files in your dbt project (typically in your seeds directory), that dbt can load into your data warehouse using the dbt seed command. Read more about seeds in the dbt docs.",source:"@site/docs/docs/configuration/seeds.md",sourceDirName:"docs/configuration",slug:"/docs/configuration/seeds",permalink:"/docs/configuration/seeds",draft:!1,editUrl:"https://github.com/dbt-athena/dbt-athena.github.io/edit/main/docs/docs/configuration/seeds.md",tags:[],version:"current",lastUpdatedAt:1706174898,formattedLastUpdatedAt:"Jan 25, 2024",frontMatter:{title:"Seeds",id:"seeds"},sidebar:"docs",previous:{title:"Iceberg table",permalink:"/docs/configuration/materializations/iceberg"},next:{title:"Snapshots",permalink:"/docs/configuration/snapshots"}},p={},c=[],m={toc:c},u="wrapper";function h(e){var t=e.components,a=(0,i.Z)(e,r);return(0,o.kt)(u,(0,n.Z)({},m,a,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("p",null,"Seeds are CSV files in your dbt project (typically in your seeds directory), that dbt can load into your data warehouse using the ",(0,o.kt)("inlineCode",{parentName:"p"},"dbt seed")," command. Read more about seeds in the ",(0,o.kt)("a",{parentName:"p",href:"https://docs.getdbt.com/docs/build/seeds"},"dbt docs"),"."))}h.isMDXComponent=!0},9081:(e,t,a)=>{"use strict";a.r(t),a.d(t,{assets:()=>c,contentTitle:()=>d,default:()=>f,frontMatter:()=>l,metadata:()=>p,toc:()=>m});var n=a(7462),i=a(3366),o=(a(7294),a(3905)),r=a(5108),s=["components"],l={title:"Snapshots",id:"snapshots"},d=void 0,p={unversionedId:"docs/configuration/snapshots",id:"docs/configuration/snapshots",title:"Snapshots",description:"Hive snapshots will undergo breaking changes in version 1.5.",source:"@site/docs/docs/configuration/snapshots.md",sourceDirName:"docs/configuration",slug:"/docs/configuration/snapshots",permalink:"/docs/configuration/snapshots",draft:!1,editUrl:"https://github.com/dbt-athena/dbt-athena.github.io/edit/main/docs/docs/configuration/snapshots.md",tags:[],version:"current",lastUpdatedAt:1706174898,formattedLastUpdatedAt:"Jan 25, 2024",frontMatter:{title:"Snapshots",id:"snapshots"},sidebar:"docs",previous:{title:"Seeds",permalink:"/docs/configuration/seeds"},next:{title:"Contract & Constraints",permalink:"/docs/configuration/contract-constraints"}},c={},m=[{value:"Detecting row changes",id:"detecting-row-changes",level:2},{value:"Timestamp strategy (recommended)",id:"timestamp-strategy-recommended",level:3},{value:"Check strategy",id:"check-strategy",level:3},{value:"Hard-deletes (opt-in)",id:"hard-deletes-opt-in",level:3},{value:"Example",id:"example",level:2}],u=function(e){return function(t){return r.warn("Component "+e+" was not imported, exported, or provided by MDXProvider as global scope"),(0,o.kt)("div",t)}},h=u("VersionBlock"),g=u("File"),k={toc:m},b="wrapper";function f(e){var t=e.components,a=(0,i.Z)(e,s);return(0,o.kt)(b,(0,n.Z)({},k,a,{components:t,mdxType:"MDXLayout"}),(0,o.kt)(h,{lastVersion:"1.4",mdxType:"VersionBlock"},(0,o.kt)("admonition",{type:"caution"},(0,o.kt)("p",{parentName:"admonition"},"Hive snapshots will undergo breaking changes in version 1.5.\nSee the ",(0,o.kt)("a",{parentName:"p",href:"/docs/migration/UPGRADE-1.5"},"migration guide")," for more details."))),(0,o.kt)(h,{firstVersion:"1.5",mdxType:"VersionBlock"},(0,o.kt)("admonition",{type:"caution"},(0,o.kt)("p",{parentName:"admonition"},"Hive snapshots underwent breaking changes in version 1.5.\nSee the ",(0,o.kt)("a",{parentName:"p",href:"/docs/migration/UPGRADE-1.5"},"migration guide")," for more details."))),(0,o.kt)("p",null,"The dbt-athena adapter supports ",(0,o.kt)("a",{parentName:"p",href:"https://docs.getdbt.com/docs/build/snapshots"},"dbt snapshots"),". Both the dbt timestamp and check strategy are supported for hive and iceberg tables. To create a snapshot, create a snapshot file in the dbt snapshots directory. If directory does not exist create one. Read more about dbt snapshots ",(0,o.kt)("a",{parentName:"p",href:"https://docs.getdbt.com/docs/build/snapshots"},"here"),"."),(0,o.kt)("h2",{id:"detecting-row-changes"},"Detecting row changes"),(0,o.kt)("h3",{id:"timestamp-strategy-recommended"},"Timestamp strategy (recommended)"),(0,o.kt)("p",null,"To use the timestamp strategy refer to the ",(0,o.kt)("a",{parentName:"p",href:"https://docs.getdbt.com/docs/build/snapshots#timestamp-strategy-recommended"},"dbt docs")),(0,o.kt)("h3",{id:"check-strategy"},"Check strategy"),(0,o.kt)("p",null,"To use the check strategy refer to the ",(0,o.kt)("a",{parentName:"p",href:"https://docs.getdbt.com/docs/build/snapshots#check-strategy"},"dbt docs")),(0,o.kt)("h3",{id:"hard-deletes-opt-in"},"Hard-deletes (opt-in)"),(0,o.kt)("p",null,"The materialization also supports invalidating hard deletes. Check the ",(0,o.kt)("a",{parentName:"p",href:"https://docs.getdbt.com/docs/build/snapshots#hard-deletes-opt-in"},"docs")," to understand the usage."),(0,o.kt)("h2",{id:"example"},"Example"),(0,o.kt)("p",null,"Place the example files below in your dbt project."),(0,o.kt)(g,{name:"seeds/base.csv",mdxType:"File"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-csv"},"id,name,some_date\n1,Easton,1981-05-20T06:46:51\n2,Lillian,1978-09-03T18:10:33\n3,Jeremiah,1982-03-11T03:59:51\n4,Nolan,1976-05-06T20:21:35\n5,Hannah,1982-06-23T05:41:26\n6,Eleanor,1991-08-10T23:12:21\n7,Lily,1971-03-29T14:58:02\n8,Jonathan,1988-02-26T02:55:24\n9,Adrian,1994-02-09T13:14:23\n10,Nora,1976-03-01T16:51:39\n"))),(0,o.kt)(g,{name:"seeds/changed.csv",mdxType:"File"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-csv"},"id,name,some_date\n4,Updated Nolan,2022-05-06T20:21:35\n5,Updated Hannah,2022-06-23T05:41:26\n6,Updated Eleanor,2022-08-10T23:12:21\n7,Lily,1971-03-29T14:58:02\n8,Jonathan,1988-02-26T02:55:24\n9,Adrian,1994-02-09T13:14:23\n10,Nora,1976-03-01T16:51:39\n11,New Mateo,2014-09-07T17:04:27\n12,New Julian,2000-02-04T11:48:30\n13,New Gabriel,2001-07-10T07:32:52\n"))),(0,o.kt)(g,{name:"snapshots/iceberg_snapshot_timestamp_strategy.sql",mdxType:"File"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sql"},"{% snapshot iceberg_snapshot_timestamp_strategy %}\n    {{ config(\n        strategy='timestamp',\n        unique_key='id',\n        updated_at='some_date',\n        table_type='iceberg',\n    )}}\n    select * from {{ ref(var('seed_name', 'base')) }}\n{% endsnapshot %}\n\n"))),(0,o.kt)(g,{name:"snapshots/hive_snapshot_timestamp_strategy_hard_deletes.sql",mdxType:"File"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sql"},"{% snapshot hive_snapshot_timestamp_strategy_hard_deletes %}\n    {{ config(\n        strategy='timestamp',\n        unique_key='id',\n        updated_at='some_date',\n        table_type='hive',\n        invalidate_hard_deletes=True,\n    )}}\n    select * from {{ ref(var('seed_name', 'base')) }}\n{% endsnapshot %}\n"))),(0,o.kt)(g,{name:"snapshots/iceberg_snapshot_check_column_strategy.sql",mdxType:"File"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sql"},"{% snapshot iceberg_snapshot_check_column_strategy %}\n    {{ config(\n        check_cols=['name'], \n        unique_key='id', \n        strategy='check',\n        table_type='iceberg',\n    ) }}\n    select * from {{ ref(var('seed_name', 'base')) }}\n{% endsnapshot %}\n"))),(0,o.kt)(g,{name:"snapshots/hive_snapshot_check_all_columns_strategy.sql",mdxType:"File"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sql"},"{% snapshot hive_snapshot_check_all_columns_strategy %}\n    {{ config(\n        check_cols='all', \n        unique_key='id', \n        strategy='check',\n        table_type='hive',\n    ) }}\n    select * from {{ ref(var('seed_name', 'base')) }}\n{% endsnapshot %}\n"))),(0,o.kt)("p",null,"Now you can test these different snapshot strategies. Replace ",(0,o.kt)("inlineCode",{parentName:"p"},"SNAPSHOT_NAME")," with one of the snapshots defined above (e.g. ",(0,o.kt)("inlineCode",{parentName:"p"},"hive_snapshot_check_all_columns_strategy"),")."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},'# Load seeds\ndbt seed\n\n# Run initial snapshot\ndbt snapshot --select SNAPSHOT_NAME\n\n# Run snapshot again, on changed source data\ndbt snapshot --select SNAPSHOT_NAME --vars "seed_name: changed"\n')),(0,o.kt)("admonition",{type:"caution"},(0,o.kt)("p",{parentName:"admonition"},"Snapshots do not support dropping columns from the source table. If you drop a column, make sure to drop the column from the snapshot as well. Another workaround is to ",(0,o.kt)("inlineCode",{parentName:"p"},"NULL")," the column in the snapshot definition to preserve history")))}f.isMDXComponent=!0},835:(e,t,a)=>{"use strict";a.r(t),a.d(t,{assets:()=>m,contentTitle:()=>p,default:()=>b,frontMatter:()=>d,metadata:()=>c,toc:()=>u});var n,i=a(7462),o=a(3366),r=(a(7294),a(3905)),s=a(5108),l=["components"],d={title:"Table configuration",id:"table-configuration"},p=void 0,c={unversionedId:"docs/configuration/table-configuration",id:"docs/configuration/table-configuration",title:"Table configuration",description:"Model configuration",source:"@site/docs/docs/configuration/table-configuration.md",sourceDirName:"docs/configuration",slug:"/docs/configuration/table-configuration",permalink:"/docs/configuration/table-configuration",draft:!1,editUrl:"https://github.com/dbt-athena/dbt-athena.github.io/edit/main/docs/docs/configuration/table-configuration.md",tags:[],version:"current",lastUpdatedAt:1706174898,formattedLastUpdatedAt:"Jan 25, 2024",frontMatter:{title:"Table configuration",id:"table-configuration"},sidebar:"docs",previous:{title:"Installation",permalink:"/docs/getting-started/installation"},next:{title:"Hive table",permalink:"/docs/configuration/materializations/hive"}},m={},u=[{value:"Model configuration",id:"model-configuration",level:2},{value:"Lakeformation configuration",id:"lakeformation-configuration",level:2},{value:"Table data location",id:"table-data-location",level:2},{value:"Incremental table models",id:"incremental-table-models",level:2},{value:"On schema change",id:"on-schema-change",level:3}],h=(n="VersionBlock",function(e){return s.warn("Component "+n+" was not imported, exported, or provided by MDXProvider as global scope"),(0,r.kt)("div",e)}),g={toc:u},k="wrapper";function b(e){var t=e.components,a=(0,o.Z)(e,l);return(0,r.kt)(k,(0,i.Z)({},g,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h2",{id:"model-configuration"},"Model configuration"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Property"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"),(0,r.kt)("th",{parentName:"tr",align:null},"Default"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"materialized")),(0,r.kt)("td",{parentName:"tr",align:null},"A table materialization like ",(0,r.kt)("inlineCode",{parentName:"td"},"table"),", ",(0,r.kt)("inlineCode",{parentName:"td"},"incremental"),", ",(0,r.kt)("a",{parentName:"td",href:"./materializations/hive-ha"},(0,r.kt)("inlineCode",{parentName:"a"},"table_hive_ha"))),(0,r.kt)("td",{parentName:"tr",align:null})),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"s3_data_naming")),(0,r.kt)("td",{parentName:"tr",align:null},"An optional naming policy for the data on S3. See ",(0,r.kt)("a",{parentName:"td",href:"#table-data-location"},"Table data location"),"."),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"schema_table_unique"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"external_location")),(0,r.kt)("td",{parentName:"tr",align:null},"If set, the full S3 path in which the table will be saved. (Does not work with Iceberg table)."),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"none"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"partitioned_by")),(0,r.kt)("td",{parentName:"tr",align:null},"An array list of columns by which the table will be partitioned. \u26a0\ufe0f ",(0,r.kt)("a",{parentName:"td",href:"https://docs.aws.amazon.com/athena/latest/ug/ctas-considerations-limitations.html#ctas-considerations-limitations-partition-and-bucket-limits"},"Limited to the creation of 100 partitions"),"."),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"none"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"bucketed_by")),(0,r.kt)("td",{parentName:"tr",align:null},"An array list of columns to bucket the data. This is ignored when using Iceberg. Example: ",(0,r.kt)("inlineCode",{parentName:"td"},"[org_id]")),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"none"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"bucket_count")),(0,r.kt)("td",{parentName:"tr",align:null},"The number of buckets for bucketing your data. This is ignored when using Iceberg. Example: ",(0,r.kt)("inlineCode",{parentName:"td"},"1"),"."),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"none"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"table_type")),(0,r.kt)("td",{parentName:"tr",align:null},"The type of table in Athena. Supports ",(0,r.kt)("inlineCode",{parentName:"td"},"hive")," or ",(0,r.kt)("inlineCode",{parentName:"td"},"iceberg")," values."),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"hive"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"format")),(0,r.kt)("td",{parentName:"tr",align:null},"The data format for the table. Supports ",(0,r.kt)("inlineCode",{parentName:"td"},"ORC"),", ",(0,r.kt)("inlineCode",{parentName:"td"},"PARQUET"),", ",(0,r.kt)("inlineCode",{parentName:"td"},"AVRO"),", ",(0,r.kt)("inlineCode",{parentName:"td"},"JSON"),", ",(0,r.kt)("inlineCode",{parentName:"td"},"TEXTFILE"),"."),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"PARQUET"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"write_compression")),(0,r.kt)("td",{parentName:"tr",align:null},"The compression type to use for any storage format that allows compression to be specified. To see which options are available, see ",(0,r.kt)("a",{parentName:"td",href:"https://docs.aws.amazon.com/athena/latest/ug/create-table-as.html"},"CREATE TABLE AS"),". Example: ",(0,r.kt)("inlineCode",{parentName:"td"},"SNAPPY"),"."),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"none"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"field_delimiter")),(0,r.kt)("td",{parentName:"tr",align:null},"Custom field delimiter. Used when the format is set to ",(0,r.kt)("inlineCode",{parentName:"td"},"TEXTFILE"),". See ",(0,r.kt)("a",{parentName:"td",href:"https://docs.aws.amazon.com/athena/latest/ug/create-table-as.html"},"CREATE TABLE AS"),". Example: ",(0,r.kt)("inlineCode",{parentName:"td"},"','")),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"none"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"table_properties")),(0,r.kt)("td",{parentName:"tr",align:null},"Additional table properties to add to the table. Valid for Iceberg only. Example: ",(0,r.kt)("inlineCode",{parentName:"td"},"{'optimize_rewrite_delete_file_threshold': '2'}")),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"none"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"native_drop")),(0,r.kt)("td",{parentName:"tr",align:null},"Relation drop operations will be performed with SQL, not direct Glue API calls. No S3 calls will be made to manage data in S3. Data in S3 will only be cleared up for Iceberg tables ",(0,r.kt)("a",{parentName:"td",href:"https://docs.aws.amazon.com/athena/latest/ug/querying-iceberg-managing-tables.html"},"see AWS docs"),". Useful in contexts where S3 access is restricted. Note that Iceberg DROP TABLE operations may timeout if they take longer than 60 seconds."),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"false"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"seed_by_insert")),(0,r.kt)("td",{parentName:"tr",align:null},"Default behaviour uploads seed data to S3. This flag will create seeds using an SQL insert statement. large seed files cannot use ",(0,r.kt)("inlineCode",{parentName:"td"},"seed_by_insert"),", as the SQL insert statement would exceed ",(0,r.kt)("a",{parentName:"td",href:"https://docs.aws.amazon.com/athena/latest/ug/service-limits.html"},"the Athena limit of 262144 bytes"),". Useful in contexts where S3 access is restricted."),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"false"))))),(0,r.kt)(h,{firstVersion:"1.5",mdxType:"VersionBlock"},(0,r.kt)("h2",{id:"lakeformation-configuration"},"Lakeformation configuration"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Property"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"),(0,r.kt)("th",{parentName:"tr",align:null},"Default"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"lf_tags_config")),(0,r.kt)("td",{parentName:"tr",align:null},"Lake Formation tags for metadata access control, to associate to the table. See detailed instructions ",(0,r.kt)("a",{parentName:"td",href:"./lakeformation"},"here")),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"none"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"lf_grants")),(0,r.kt)("td",{parentName:"tr",align:null},"Lake Formation data cell filters configuration. See detailed instructions ",(0,r.kt)("a",{parentName:"td",href:"./lakeformation"},"here")),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"none")))))),(0,r.kt)("h2",{id:"table-data-location"},"Table data location"),(0,r.kt)("p",null,"The S3 location in which table data is saved is determined by:"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"If ",(0,r.kt)("inlineCode",{parentName:"li"},"external_location")," is defined, that value is used."),(0,r.kt)("li",{parentName:"ol"},"If ",(0,r.kt)("inlineCode",{parentName:"li"},"s3_data_dir")," is defined, the path is determined by this value and ",(0,r.kt)("inlineCode",{parentName:"li"},"s3_data_naming"),"."),(0,r.kt)("li",{parentName:"ol"},"If no ",(0,r.kt)("inlineCode",{parentName:"li"},"s3_data_dir")," is defined, the data is stored under ",(0,r.kt)("inlineCode",{parentName:"li"},"s3_staging_dir/tables/"),".")),(0,r.kt)("p",null,"The options for ",(0,r.kt)("inlineCode",{parentName:"p"},"s3_data_naming")," are:"),(0,r.kt)(h,{lastVersion:"1.4",mdxType:"VersionBlock"},(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"strong"},"uuid")),": ",(0,r.kt)("inlineCode",{parentName:"li"},"{s3_data_dir}/{uuid4()}/")))),(0,r.kt)(h,{firstVersion:"1.5",mdxType:"VersionBlock"},(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"strong"},"unique")),": ",(0,r.kt)("inlineCode",{parentName:"li"},"{s3_data_dir}/{uuid4()}/")))),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"strong"},"table_table")),": ",(0,r.kt)("inlineCode",{parentName:"li"},"{s3_data_dir}/{table}/")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"strong"},"table_unique")),": ",(0,r.kt)("inlineCode",{parentName:"li"},"{s3_data_dir}/{table}/{uuid4()}/")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"strong"},"schema_table")),": ",(0,r.kt)("inlineCode",{parentName:"li"},"{s3_data_dir}/{schema}/{table}/")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"strong"},"schema_table_unique")),": ",(0,r.kt)("inlineCode",{parentName:"li"},"{s3_data_dir}/{schema}/{table}/{uuid4()}/"))),(0,r.kt)(h,{firstVersion:"1.5",mdxType:"VersionBlock"},(0,r.kt)("admonition",{title:"Unique and UUID naming strategy",type:"caution"},(0,r.kt)("p",{parentName:"admonition"},"In the past, the ",(0,r.kt)("inlineCode",{parentName:"p"},"unique")," strategy was named ",(0,r.kt)("inlineCode",{parentName:"p"},"uuid"),". We changed that to ensure consistency in our strategies naming."))),(0,r.kt)("p",null,"It's possible to set the ",(0,r.kt)("inlineCode",{parentName:"p"},"s3_data_naming")," globally in the ",(0,r.kt)("inlineCode",{parentName:"p"},"profile.yml"),", set it for a group of models in the ",(0,r.kt)("inlineCode",{parentName:"p"},"dbt_project.yml")," or overwrite the value for a specific model in the config block."),(0,r.kt)("admonition",{title:"Workgroup with default output location",type:"caution"},(0,r.kt)("p",{parentName:"admonition"},"When using an Athena workgroup with a default output location configured, ",(0,r.kt)("inlineCode",{parentName:"p"},"s3_data_naming")," and any configured buckets are ignored and the location configured in the workgroup is used.")),(0,r.kt)("h2",{id:"incremental-table-models"},"Incremental table models"),(0,r.kt)("p",null,"dbt-athena supports ",(0,r.kt)("a",{parentName:"p",href:"https://docs.getdbt.com/docs/build/incremental-models"},"incremental models"),". These strategies are supported:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"insert_overwrite")," (default): The insert overwrite strategy deletes the overlapping partitions from the destination table, and then inserts the new records from the source. This strategy depends on the ",(0,r.kt)("inlineCode",{parentName:"li"},"partitioned_by")," keyword! If no partitions are defined, dbt will fall back to the ",(0,r.kt)("inlineCode",{parentName:"li"},"append")," strategy."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"append"),": Insert new records without updating, deleting or overwriting any existing data. There might be duplicate data (e.g. great for log or historical data)."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"merge"),": Conditionally updates, deletes, or inserts rows into an Iceberg table. Used in combination with ",(0,r.kt)("inlineCode",{parentName:"li"},"unique_key"),". \u26a0\ufe0f Only available when using ",(0,r.kt)("a",{parentName:"li",href:"docs/configuration/materializations/iceberg"},"Iceberg"),".")),(0,r.kt)("h3",{id:"on-schema-change"},"On schema change"),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"on_schema_change")," is an option to reflect changes of schema in incremental models.\nThe following options are supported:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"ignore")," (default)"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"fail")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"append_new_columns")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"sync_all_columns"))),(0,r.kt)("p",null,"In detail, please refer to ",(0,r.kt)("a",{parentName:"p",href:"https://docs.getdbt.com/docs/build/incremental-models#what-if-the-columns-of-my-incremental-model-change"},"dbt docs"),"."))}b.isMDXComponent=!0},9482:(e,t,a)=>{"use strict";a.r(t),a.d(t,{assets:()=>m,contentTitle:()=>p,default:()=>b,frontMatter:()=>d,metadata:()=>c,toc:()=>u});var n,i=a(7462),o=a(3366),r=(a(7294),a(3905)),s=a(5108),l=["components"],d={title:"Become a contributor",sidebar_label:"Contributing",id:"contributing"},p=void 0,c={unversionedId:"docs/contributing/contributing",id:"docs/contributing/contributing",title:"Become a contributor",description:"A community-owned project",source:"@site/docs/docs/contributing/contributing.md",sourceDirName:"docs/contributing",slug:"/docs/contributing/",permalink:"/docs/contributing/",draft:!1,editUrl:"https://github.com/dbt-athena/dbt-athena.github.io/edit/main/docs/docs/contributing/contributing.md",tags:[],version:"current",lastUpdatedAt:1706174898,formattedLastUpdatedAt:"Jan 25, 2024",frontMatter:{title:"Become a contributor",sidebar_label:"Contributing",id:"contributing"},sidebar:"docs",previous:{title:"Upgrade to version 1.5",permalink:"/docs/migration/UPGRADE-1.5"},next:{title:"Local development",permalink:"/docs/contributing/local-development"}},m={},u=[{value:"A community-owned project",id:"a-community-owned-project",level:2}],h=(n="Card",function(e){return s.warn("Component "+n+" was not imported, exported, or provided by MDXProvider as global scope"),(0,r.kt)("div",e)}),g={toc:u},k="wrapper";function b(e){var t=e.components,a=(0,o.Z)(e,l);return(0,r.kt)(k,(0,i.Z)({},g,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h2",{id:"a-community-owned-project"},"A community-owned project"),(0,r.kt)("p",null,"The dbt-athena adapter is a community-owned adapter on Github, which means that the code is maintained and developed by a group of individuals rather than a single company or organization. This approach of community ownership ensures that the adapter is not dependent on a single entity, reducing the risk of the code becoming unsupported or abandoned."),(0,r.kt)("p",null,"Community ownership also fosters collaboration, encourages transparency, and helps to ensure that the code is developed and maintained with the needs of the community in mind. The dbt-athena adapter is an excellent example of how the power of the open-source community can be harnessed to develop high-quality, reliable, and sustainable software."),(0,r.kt)("p",null,"Community ownership also means that there is no single point of failure in terms of the code's development and maintenance. With multiple contributors and maintainers, the risk of the project being abandoned or becoming unsupported due to changes in the ownership structure is reduced. Additionally, the diverse perspectives and expertise of the community can result in a more robust and feature-rich codebase."),(0,r.kt)("div",{className:"grid--2-col"},(0,r.kt)(h,{title:"Local development setup",body:"Learn how to setup and debug the project locally",link:"docs/contributing/local-development",icon:"computer",mdxType:"Card"}),(0,r.kt)(h,{title:"Online community building",body:"Getting involved in the dbt Community Slack (#db-athena) is one of the best entry points for contributing. Share your knowledge about Athena and dbt-athena and learn from others.",link:"https://www.getdbt.com/community/join-the-community/",icon:"slack",mdxType:"Card"})))}b.isMDXComponent=!0},9697:(e,t,a)=>{"use strict";a.r(t),a.d(t,{assets:()=>m,contentTitle:()=>p,default:()=>b,frontMatter:()=>d,metadata:()=>c,toc:()=>u});var n,i=a(7462),o=a(3366),r=(a(7294),a(3905)),s=a(5108),l=["components"],d={title:"Local development",id:"local-development"},p=void 0,c={unversionedId:"docs/contributing/local-development",id:"docs/contributing/local-development",title:"Local development",description:"Using virtual environments",source:"@site/docs/docs/contributing/local-development.md",sourceDirName:"docs/contributing",slug:"/docs/contributing/local-development",permalink:"/docs/contributing/local-development",draft:!1,editUrl:"https://github.com/dbt-athena/dbt-athena.github.io/edit/main/docs/docs/contributing/local-development.md",tags:[],version:"current",lastUpdatedAt:1706174898,formattedLastUpdatedAt:"Jan 25, 2024",frontMatter:{title:"Local development",id:"local-development"},sidebar:"docs",previous:{title:"Contributing",permalink:"/docs/contributing/"},next:{title:"Frequently asked questions",permalink:"/docs/faqs"}},m={},u=[{value:"Using virtual environments",id:"using-virtual-environments",level:2},{value:"Install a specific package of dbt-athena in your project",id:"install-a-specific-package-of-dbt-athena-in-your-project",level:2},{value:"Local package",id:"local-package",level:3},{value:"Git branch",id:"git-branch",level:3},{value:"Unit tests",id:"unit-tests",level:2}],h=(n="File",function(e){return s.warn("Component "+n+" was not imported, exported, or provided by MDXProvider as global scope"),(0,r.kt)("div",e)}),g={toc:u},k="wrapper";function b(e){var t=e.components,a=(0,o.Z)(e,l);return(0,r.kt)(k,(0,i.Z)({},g,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h2",{id:"using-virtual-environments"},"Using virtual environments"),(0,r.kt)("p",null,"Virtualenv is the most common and easy to install tool for virtual environments. It\u2019s a great tool for beginners."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-terminal"},"python3 -m venv dbt-env             # create the environment\nsource dbt-env/bin/activate         # activate the environment for Mac and Linux\ndbt-env\\Scripts\\activate            # activate the environment for Windows\n")),(0,r.kt)("p",null,"If you install dbt in a virtual environment, you need to reactivate that same virtual environment each time you create a shell window or session."),(0,r.kt)("h2",{id:"install-a-specific-package-of-dbt-athena-in-your-project"},"Install a specific package of dbt-athena in your project"),(0,r.kt)("h3",{id:"local-package"},"Local package"),(0,r.kt)("p",null,"When you are experiencing a bug, it's often very useful to run against a local version of dbt-athena, with the ability to add debug logs and fixes."),(0,r.kt)(h,{name:"requirements.txt",mdxType:"File"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-text"},"-e /Users/username/Code/dbt-athena\n"))),(0,r.kt)("p",null,"Then, run ",(0,r.kt)("inlineCode",{parentName:"p"},"pip install -r requirements.txt")),(0,r.kt)("h3",{id:"git-branch"},"Git branch"),(0,r.kt)("p",null,"To test a specific branch in your project, add the following to your ",(0,r.kt)("inlineCode",{parentName:"p"},"requirements.txt")," file:"),(0,r.kt)(h,{name:"requirements.txt",mdxType:"File"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-text"},"git+https://github.com/dbt-athena/dbt-athena.git@main#dbt-athena-community\n"))),(0,r.kt)("p",null,"Then, run ",(0,r.kt)("inlineCode",{parentName:"p"},"pip install -r requirements.txt")),(0,r.kt)("h2",{id:"unit-tests"},"Unit tests"),(0,r.kt)("p",null,"To run the unit tests using Pytest in dbt-athena, follow these instructions:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-terminal"},"make install_deps\nmake unit_test\n")))}b.isMDXComponent=!0},9641:(e,t,a)=>{"use strict";a.r(t),a.d(t,{assets:()=>p,contentTitle:()=>l,default:()=>h,frontMatter:()=>s,metadata:()=>d,toc:()=>c});var n=a(7462),i=a(3366),o=(a(7294),a(3905)),r=["components"],s={title:"Installing dbt-athena",id:"installation",sidebar_label:"Installation"},l=void 0,d={unversionedId:"docs/getting-started/installation",id:"docs/getting-started/installation",title:"Installing dbt-athena",description:"Prerequisites",source:"@site/docs/docs/getting-started/installation.md",sourceDirName:"docs/getting-started",slug:"/docs/getting-started/installation",permalink:"/docs/getting-started/installation",draft:!1,editUrl:"https://github.com/dbt-athena/dbt-athena.github.io/edit/main/docs/docs/getting-started/installation.md",tags:[],version:"current",lastUpdatedAt:1706174898,formattedLastUpdatedAt:"Jan 25, 2024",frontMatter:{title:"Installing dbt-athena",id:"installation",sidebar_label:"Installation"},sidebar:"docs",previous:{title:"IAM permissions",permalink:"/docs/getting-started/prerequisites/iam-permissions"},next:{title:"Table configuration",permalink:"/docs/configuration/table-configuration"}},p={},c=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Install dbt-athena",id:"install-dbt-athena",level:2},{value:"Configuring dbt-athena",id:"configuring-dbt-athena",level:3}],m={toc:c},u="wrapper";function h(e){var t=e.components,a=(0,i.Z)(e,r);return(0,o.kt)(u,(0,n.Z)({},m,a,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h2",{id:"prerequisites"},"Prerequisites"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Installed dbt Core using the ",(0,o.kt)("a",{parentName:"li",href:"https://docs.getdbt.com/docs/core/installation"},"installation instructions")," for your operating system + a dbt project initialized."),(0,o.kt)("li",{parentName:"ul"},"A working Athena setup.")),(0,o.kt)("h2",{id:"install-dbt-athena"},"Install dbt-athena"),(0,o.kt)("p",null,"Pip is the easiest way to install the ",(0,o.kt)("a",{parentName:"p",href:"https://pypi.org/project/dbt-athena-community/"},"dbt-athena-community package")," in your dbt project:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-terminal"},"pip install dbt-athena-community\n")),(0,o.kt)("h3",{id:"configuring-dbt-athena"},"Configuring dbt-athena"),(0,o.kt)("p",null,"A dbt profile can be configured in the ",(0,o.kt)("inlineCode",{parentName:"p"},"profiles.yml")," to run against AWS Athena using the following configuration:"),(0,o.kt)("table",null,(0,o.kt)("thead",{parentName:"table"},(0,o.kt)("tr",{parentName:"thead"},(0,o.kt)("th",{parentName:"tr",align:null},"Option"),(0,o.kt)("th",{parentName:"tr",align:null},"Description"),(0,o.kt)("th",{parentName:"tr",align:null},"Required?"),(0,o.kt)("th",{parentName:"tr",align:null},"Example"))),(0,o.kt)("tbody",{parentName:"table"},(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},(0,o.kt)("inlineCode",{parentName:"td"},"s3_staging_dir")),(0,o.kt)("td",{parentName:"tr",align:null},"S3 location to store Athena query results and metadata"),(0,o.kt)("td",{parentName:"tr",align:null},"Required"),(0,o.kt)("td",{parentName:"tr",align:null},(0,o.kt)("inlineCode",{parentName:"td"},"s3://bucket/dbt/"))),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},(0,o.kt)("inlineCode",{parentName:"td"},"s3_data_dir")),(0,o.kt)("td",{parentName:"tr",align:null},"Prefix for storing tables, if different from the connection's ",(0,o.kt)("inlineCode",{parentName:"td"},"s3_staging_dir")),(0,o.kt)("td",{parentName:"tr",align:null},"Optional"),(0,o.kt)("td",{parentName:"tr",align:null},(0,o.kt)("inlineCode",{parentName:"td"},"s3://bucket2/dbt/"))),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},(0,o.kt)("inlineCode",{parentName:"td"},"s3_data_naming")),(0,o.kt)("td",{parentName:"tr",align:null},"How to generate table paths in ",(0,o.kt)("inlineCode",{parentName:"td"},"s3_data_dir"),". Default: ",(0,o.kt)("inlineCode",{parentName:"td"},"schema_table_unique"),". See ",(0,o.kt)("a",{parentName:"td",href:"docs/configuration/table-configuration"},"Table Configuration")),(0,o.kt)("td",{parentName:"tr",align:null},"Optional"),(0,o.kt)("td",{parentName:"tr",align:null},(0,o.kt)("inlineCode",{parentName:"td"},"schema_table_unique"))),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},(0,o.kt)("inlineCode",{parentName:"td"},"region_name")),(0,o.kt)("td",{parentName:"tr",align:null},"AWS region of your Athena instance"),(0,o.kt)("td",{parentName:"tr",align:null},"Required"),(0,o.kt)("td",{parentName:"tr",align:null},(0,o.kt)("inlineCode",{parentName:"td"},"eu-central-1"))),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},(0,o.kt)("inlineCode",{parentName:"td"},"schema")),(0,o.kt)("td",{parentName:"tr",align:null},"Specify the schema (Athena database) to build models into (lowercase ",(0,o.kt)("strong",{parentName:"td"},"only"),")"),(0,o.kt)("td",{parentName:"tr",align:null},"Required"),(0,o.kt)("td",{parentName:"tr",align:null},(0,o.kt)("inlineCode",{parentName:"td"},"dbt"))),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},(0,o.kt)("inlineCode",{parentName:"td"},"database")),(0,o.kt)("td",{parentName:"tr",align:null},"Specify the database (Data catalog) to build models into (lowercase ",(0,o.kt)("strong",{parentName:"td"},"only"),")"),(0,o.kt)("td",{parentName:"tr",align:null},"Required"),(0,o.kt)("td",{parentName:"tr",align:null},(0,o.kt)("inlineCode",{parentName:"td"},"awsdatacatalog"))),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},(0,o.kt)("inlineCode",{parentName:"td"},"poll_interval")),(0,o.kt)("td",{parentName:"tr",align:null},"Interval in seconds to use for polling the status of query results in Athena. Default: ",(0,o.kt)("inlineCode",{parentName:"td"},"1")),(0,o.kt)("td",{parentName:"tr",align:null},"Optional"),(0,o.kt)("td",{parentName:"tr",align:null},(0,o.kt)("inlineCode",{parentName:"td"},"5"))),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},(0,o.kt)("inlineCode",{parentName:"td"},"aws_profile_name")),(0,o.kt)("td",{parentName:"tr",align:null},"Profile to use from your AWS shared credentials file."),(0,o.kt)("td",{parentName:"tr",align:null},"Optional"),(0,o.kt)("td",{parentName:"tr",align:null},(0,o.kt)("inlineCode",{parentName:"td"},"my-profile"))),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},(0,o.kt)("inlineCode",{parentName:"td"},"aws_access_key_id")),(0,o.kt)("td",{parentName:"tr",align:null},"AWS access key to sign AWS API requests. This is optional, as ",(0,o.kt)("a",{parentName:"td",href:"https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html"},"credentials are determined automatically based on the aws cli and boto3 conventions")," and stored login info."),(0,o.kt)("td",{parentName:"tr",align:null},"Optional"),(0,o.kt)("td",{parentName:"tr",align:null},(0,o.kt)("inlineCode",{parentName:"td"},"AKIAIOSFODNN7EXAMPLE"))),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},(0,o.kt)("inlineCode",{parentName:"td"},"aws_secret_access_key")),(0,o.kt)("td",{parentName:"tr",align:null},"AWS secret access key to sign AWS API requests. This is optional, as ",(0,o.kt)("a",{parentName:"td",href:"https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html"},"credentials are determined automatically based on the aws cli and boto3 conventions")," and stored login info."),(0,o.kt)("td",{parentName:"tr",align:null},"Optional"),(0,o.kt)("td",{parentName:"tr",align:null},(0,o.kt)("inlineCode",{parentName:"td"},"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"))),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},(0,o.kt)("inlineCode",{parentName:"td"},"work_group")),(0,o.kt)("td",{parentName:"tr",align:null},"Identifier of Athena workgroup"),(0,o.kt)("td",{parentName:"tr",align:null},"Optional"),(0,o.kt)("td",{parentName:"tr",align:null},(0,o.kt)("inlineCode",{parentName:"td"},"my-custom-workgroup"))),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},(0,o.kt)("inlineCode",{parentName:"td"},"num_retries")),(0,o.kt)("td",{parentName:"tr",align:null},"Number of times to retry a failing query. Default: ",(0,o.kt)("inlineCode",{parentName:"td"},"5")),(0,o.kt)("td",{parentName:"tr",align:null},"Optional"),(0,o.kt)("td",{parentName:"tr",align:null},(0,o.kt)("inlineCode",{parentName:"td"},"3"))),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},(0,o.kt)("inlineCode",{parentName:"td"},"lf_tags")),(0,o.kt)("td",{parentName:"tr",align:null},"LF tags to apply to any database created by dbt"),(0,o.kt)("td",{parentName:"tr",align:null},"Optional"),(0,o.kt)("td",{parentName:"tr",align:null},(0,o.kt)("inlineCode",{parentName:"td"},'{"origin": "dbt", "team": "analytics"}'))))),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Example profiles.yml entry:")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},"athena:\n  target: dev\n  outputs:\n    dev:\n      type: athena\n      work_group: primary\n      s3_staging_dir: s3://aws-athena-query-results/dbt/\n      s3_data_dir: s3://your_s3_bucket/dbt/\n      s3_data_naming: schema_table_unique\n      region_name: eu-central-1\n      database: awsdatacatalog\n      schema: dbt\n      aws_profile_name: my-profile\n      lf_tags:\n        origin: dbt\n        team: analytics\n      threads: 8\n")),(0,o.kt)("p",null,(0,o.kt)("em",{parentName:"p"},"Additional information")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"threads")," is supported"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"database")," and ",(0,o.kt)("inlineCode",{parentName:"li"},"catalog")," can be used interchangeably")))}h.isMDXComponent=!0},4047:(e,t,a)=>{"use strict";a.r(t),a.d(t,{assets:()=>p,contentTitle:()=>l,default:()=>h,frontMatter:()=>s,metadata:()=>d,toc:()=>c});var n=a(7462),i=a(3366),o=(a(7294),a(3905)),r=["components"],s={title:"AWS resources",id:"aws-resources"},l=void 0,d={unversionedId:"docs/getting-started/prerequisites/aws-resources",id:"docs/getting-started/prerequisites/aws-resources",title:"AWS resources",description:"To get started, you will need an S3 bucket, for instance my-bucket, and an Athena database:",source:"@site/docs/docs/getting-started/prerequisites/aws-resources.md",sourceDirName:"docs/getting-started/prerequisites",slug:"/docs/getting-started/prerequisites/aws-resources",permalink:"/docs/getting-started/prerequisites/aws-resources",draft:!1,editUrl:"https://github.com/dbt-athena/dbt-athena.github.io/edit/main/docs/docs/getting-started/prerequisites/aws-resources.md",tags:[],version:"current",lastUpdatedAt:1706174898,formattedLastUpdatedAt:"Jan 25, 2024",frontMatter:{title:"AWS resources",id:"aws-resources"},sidebar:"docs",previous:{title:"Introduction",permalink:"/"},next:{title:"IAM permissions",permalink:"/docs/getting-started/prerequisites/iam-permissions"}},p={},c=[],m={toc:c},u="wrapper";function h(e){var t=e.components,a=(0,i.Z)(e,r);return(0,o.kt)(u,(0,n.Z)({},m,a,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("p",null,"To get started, you will need an S3 bucket, for instance ",(0,o.kt)("inlineCode",{parentName:"p"},"my-bucket"),", and an Athena database:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sql"},"CREATE DATABASE IF NOT EXISTS analytics_dev\nCOMMENT 'Analytics models generated by dbt (development)'\nLOCATION 's3://my-bucket/'\nWITH DBPROPERTIES ('creator'='Foo Bar', 'email'='foo@bar.com');\n")),(0,o.kt)("p",null,"If the database does not exist, then dbt will attempt to create it automatically."),(0,o.kt)("admonition",{title:"AWS Glue",type:"info"},(0,o.kt)("p",{parentName:"admonition"},"You can also use ",(0,o.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/athena/latest/ug/glue-athena.html"},"AWS Glue")," to create and manage your Athena databases.")))}h.isMDXComponent=!0},4632:(e,t,a)=>{"use strict";a.r(t),a.d(t,{assets:()=>p,contentTitle:()=>l,default:()=>h,frontMatter:()=>s,metadata:()=>d,toc:()=>c});var n=a(7462),i=a(3366),o=(a(7294),a(3905)),r=["components"],s={title:"IAM permissions",id:"iam-permissions"},l=void 0,d={unversionedId:"docs/getting-started/prerequisites/iam-permissions",id:"docs/getting-started/prerequisites/iam-permissions",title:"IAM permissions",description:"Athena IAM permissions",source:"@site/docs/docs/getting-started/prerequisites/iam-permissions.md",sourceDirName:"docs/getting-started/prerequisites",slug:"/docs/getting-started/prerequisites/iam-permissions",permalink:"/docs/getting-started/prerequisites/iam-permissions",draft:!1,editUrl:"https://github.com/dbt-athena/dbt-athena.github.io/edit/main/docs/docs/getting-started/prerequisites/iam-permissions.md",tags:[],version:"current",lastUpdatedAt:1706174898,formattedLastUpdatedAt:"Jan 25, 2024",frontMatter:{title:"IAM permissions",id:"iam-permissions"},sidebar:"docs",previous:{title:"AWS resources",permalink:"/docs/getting-started/prerequisites/aws-resources"},next:{title:"Installation",permalink:"/docs/getting-started/installation"}},p={},c=[{value:"Athena IAM permissions",id:"athena-iam-permissions",level:2},{value:"Glue IAM permissions",id:"glue-iam-permissions",level:2},{value:"S3 IAM permissions",id:"s3-iam-permissions",level:2},{value:"Lake Formation",id:"lake-formation",level:2}],m={toc:c},u="wrapper";function h(e){var t=e.components,a=(0,i.Z)(e,r);return(0,o.kt)(u,(0,n.Z)({},m,a,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h2",{id:"athena-iam-permissions"},"Athena IAM permissions"),(0,o.kt)("p",null,"Athena permissions that are required to run queries:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-json"},'"athena:StartQueryExecution",\n"athena:GetQueryResults",\n"athena:GetWorkGroup",\n"athena:StopQueryExecution",\n"athena:GetQueryExecution",\n')),(0,o.kt)("h2",{id:"glue-iam-permissions"},"Glue IAM permissions"),(0,o.kt)("p",null,"dbt-athena uses the AWS Glue API to fetch metadata. You will need to set these permissions on the Glue databases you are reading from:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-json"},'"glue:GetDatabase",\n"glue:GetDatabases",\n"glue:GetTable",\n"glue:GetTables",\n"glue:GetTableVersions",\n"glue:GetPartition",\n"glue:GetPartitions",\n')),(0,o.kt)("p",null,"You will need these permissions on the glue databases you are writing to:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-json"},'"glue:CreateDatabase",  -- Indeed, in case the Athena database does not exist, DBT will try to create it for you.\n"glue:GetDatabase",\n"glue:GetDatabases",\n"glue:GetTable",\n"glue:GetTables",\n"glue:GetPartition",\n"glue:GetPartitions",\n"glue:BatchCreatePartition",\n"glue:BatchUpdatePartition",\n"glue:BatchDeletePartition",\n"glue:BatchDeleteTable",\n"glue:BatchDeleteTableVersion",\n"glue:CreatePartition",\n"glue:UpdatePartition",\n"glue:DeletePartition",\n"glue:CreateTable",\n"glue:UpdateTable",\n"glue:DeleteTable",\n"glue:DeleteTableVersion",\n')),(0,o.kt)("h2",{id:"s3-iam-permissions"},"S3 IAM permissions"),(0,o.kt)("p",null,"You will need these permissions on the S3 buckets that dbt-athena reads from:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-json"},'"s3:GetObject",\n"s3:GetBucketLocation",\n"s3:ListBucket",\n"s3:ListBucketMultipartUploads",\n"s3:ListMultipartUploadParts",\n')),(0,o.kt)("p",null,"You will need these permissions on the S3 buckets you are writing to (buckets defined in ",(0,o.kt)("inlineCode",{parentName:"p"},"s3_staging_dir")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"s3_data_dir"),"):"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-json"},'"s3:GetObject",\n"s3:GetBucketLocation",\n"s3:ListBucket",\n"s3:ListBucketMultipartUploads",\n"s3:ListMultipartUploadParts",\n"s3:AbortMultipartUpload",\n"s3:PutObject",\n"s3:DeleteObject",\n')),(0,o.kt)("p",null,"If your buckets are encrypted using KMS, you will need these permissions on every KMS key of the buckets:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-json"},'"kms:GenerateDataKey*",\n"kms:DescribeKey",\n"kms:Decrypt",\n')),(0,o.kt)("h2",{id:"lake-formation"},"Lake Formation"),(0,o.kt)("p",null,"If you are using databases managed by AWS Lake Formation, then you need to set these permissions on the role.:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-json"},'"lakeformation:GetDataAccess",\n')))}h.isMDXComponent=!0},2057:(e,t,a)=>{"use strict";a.r(t),a.d(t,{assets:()=>p,contentTitle:()=>l,default:()=>h,frontMatter:()=>s,metadata:()=>d,toc:()=>c});var n=a(7462),i=a(3366),o=(a(7294),a(3905)),r=["components"],s={title:"What is dbt-athena?",id:"introduction",sidebar_label:"Introduction",slug:"/"},l=void 0,d={unversionedId:"docs/introduction",id:"docs/introduction",title:"What is dbt-athena?",description:"dbt-athena is a community-owned adapter for dbt Core. The dbt-athena package contains all the code to enable dbt to work with AWS Athena and transform data using SQL.",source:"@site/docs/docs/introduction.md",sourceDirName:"docs",slug:"/",permalink:"/",draft:!1,editUrl:"https://github.com/dbt-athena/dbt-athena.github.io/edit/main/docs/docs/introduction.md",tags:[],version:"current",lastUpdatedAt:1706174898,formattedLastUpdatedAt:"Jan 25, 2024",frontMatter:{title:"What is dbt-athena?",id:"introduction",sidebar_label:"Introduction",slug:"/"},sidebar:"docs",next:{title:"AWS resources",permalink:"/docs/getting-started/prerequisites/aws-resources"}},p={},c=[{value:"Features of dbt-athena",id:"features-of-dbt-athena",level:2}],m={toc:c},u="wrapper";function h(e){var t=e.components,a=(0,i.Z)(e,r);return(0,o.kt)(u,(0,n.Z)({},m,a,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("p",null,"dbt-athena is a community-owned adapter for dbt Core. The dbt-athena package contains all the code to enable dbt to work with ",(0,o.kt)("a",{parentName:"p",href:"https://aws.amazon.com/athena/"},"AWS Athena")," and transform data using SQL."),(0,o.kt)("p",null,"In essence, the dbt-athena adapter will transform existing data in Athena by leveraging the ",(0,o.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/athena/latest/ug/create-table-as.html"},(0,o.kt)("inlineCode",{parentName:"a"},"CREATE TABLE AS"))," or ",(0,o.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/athena/latest/ug/create-view.html"},(0,o.kt)("inlineCode",{parentName:"a"},"CREATE VIEW"))," SQL queries in AWS Athena."),(0,o.kt)("h2",{id:"features-of-dbt-athena"},"Features of dbt-athena"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"\u2705 Supports dbt version ",(0,o.kt)("inlineCode",{parentName:"li"},"1.6.*")),(0,o.kt)("li",{parentName:"ul"},"\u2705 Supports ",(0,o.kt)("a",{parentName:"li",href:"https://docs.getdbt.com/docs/building-a-dbt-project/seeds"},"dbt seeds")),(0,o.kt)("li",{parentName:"ul"},"\u2705 Supports ",(0,o.kt)("a",{parentName:"li",href:"https://docs.getdbt.com/docs/build/snapshots"},"dbt snapshots")),(0,o.kt)("li",{parentName:"ul"},"\u2705 Supports ",(0,o.kt)("a",{parentName:"li",href:"https://docs.getdbt.com/docs/build/materializations#table"},"table materialization")," using Hive tables (Athena v2/v3) and ",(0,o.kt)("a",{parentName:"li",href:"https://docs.aws.amazon.com/athena/latest/ug/querying-iceberg.html"},"Iceberg tables")," (Athena v3)"),(0,o.kt)("li",{parentName:"ul"},"\u2705 Supports ",(0,o.kt)("a",{parentName:"li",href:"https://docs.getdbt.com/docs/build/incremental-models"},"incremental models")," for Iceberg (",(0,o.kt)("inlineCode",{parentName:"li"},"merge")," and ",(0,o.kt)("inlineCode",{parentName:"li"},"append"),") and Hive tables (",(0,o.kt)("inlineCode",{parentName:"li"},"insert_overwrite")," and ",(0,o.kt)("inlineCode",{parentName:"li"},"append"),").")),(0,o.kt)("p",null,"Not supported yet: ",(0,o.kt)("a",{parentName:"p",href:"https://docs.getdbt.com/docs/build/python-models#configuring-python-models"},"Python models"),", ",(0,o.kt)("a",{parentName:"p",href:"https://docs.getdbt.com/reference/resource-configs/persist_docs"},"persist docs")," for views."))}h.isMDXComponent=!0},7526:(e,t,a)=>{"use strict";a.r(t),a.d(t,{assets:()=>p,contentTitle:()=>l,default:()=>h,frontMatter:()=>s,metadata:()=>d,toc:()=>c});var n=a(7462),i=a(3366),o=(a(7294),a(3905)),r=["components"],s={title:"Known issues",id:"known-issues"},l=void 0,d={unversionedId:"docs/known-issues",id:"docs/known-issues",title:"Known issues",description:"- Incremental Iceberg models: sync all columns on schema change can't remove columns used as partitioning. The only way, from a dbt perspective, is to do a full-refresh of the incremental model.",source:"@site/docs/docs/known-issues.md",sourceDirName:"docs",slug:"/docs/known-issues",permalink:"/docs/known-issues",draft:!1,editUrl:"https://github.com/dbt-athena/dbt-athena.github.io/edit/main/docs/docs/known-issues.md",tags:[],version:"current",lastUpdatedAt:1706174898,formattedLastUpdatedAt:"Jan 25, 2024",frontMatter:{title:"Known issues",id:"known-issues"},sidebar:"docs",previous:{title:"Lakeformation",permalink:"/docs/configuration/lakeformation"},next:{title:"Upgrade to version 1.5",permalink:"/docs/migration/UPGRADE-1.5"}},p={},c=[{value:"An error occurred (InvalidIdentityToken) when calling the AssumeRoleWithWebIdentity operation",id:"an-error-occurred-invalididentitytoken-when-calling-the-assumerolewithwebidentity-operation",level:2}],m={toc:c},u="wrapper";function h(e){var t=e.components,a=(0,i.Z)(e,r);return(0,o.kt)(u,(0,n.Z)({},m,a,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Incremental Iceberg models: sync all columns on schema change can't remove columns used as partitioning. The only way, from a dbt perspective, is to do a full-refresh of the incremental model."),(0,o.kt)("li",{parentName:"ul"},"Tables, schemas and database should only be lowercase")),(0,o.kt)("h2",{id:"an-error-occurred-invalididentitytoken-when-calling-the-assumerolewithwebidentity-operation"},"An error occurred (InvalidIdentityToken) when calling the AssumeRoleWithWebIdentity operation"),(0,o.kt)("p",null,"When using db-athena with Single Sign On (SSO), the mechanism you use to assume the target role can cause problems. You can provide the token in ",(0,o.kt)("inlineCode",{parentName:"p"},"~/.aws/config")," as follows:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-toml"},"[profile web-identity]\nrole_arn=arn:aws:iam:...\nweb_identity_token_file=/path/to/a/token\n")),(0,o.kt)("p",null,"This approach has been known to cause an error during your dbt execution (",(0,o.kt)("a",{parentName:"p",href:"https://github.com/dbt-athena/dbt-athena/issues/259"},"issue#259"),"):"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"An error occurred (InvalidIdentityToken) when calling the AssumeRoleWithWebIdentity operation: Couldn't retrieve verification key from your identity provider, please reference AssumeRoleWithWebIdentity documentation for requirements\"\n")),(0,o.kt)("p",null,"A solution is to explicitly assume the role before your dbt execution, rather than letting the underlying libraries take care of it. Call ",(0,o.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/cli/latest/reference/sts/assume-role-with-web-identity.html"},(0,o.kt)("inlineCode",{parentName:"a"},"aws sts assume-role-with-web-identity"))," with an appropriate duration."))}h.isMDXComponent=!0},7368:(e,t,a)=>{"use strict";a.r(t),a.d(t,{assets:()=>p,contentTitle:()=>l,default:()=>h,frontMatter:()=>s,metadata:()=>d,toc:()=>c});var n=a(7462),i=a(3366),o=(a(7294),a(3905)),r=["components"],s={title:"Upgrade to version 1.5",id:"UPGRADE-1.5"},l=void 0,d={unversionedId:"docs/migration/UPGRADE-1.5",id:"docs/migration/UPGRADE-1.5",title:"Upgrade to version 1.5",description:"Highly available Hive table",source:"@site/docs/docs/migration/UPGRADE-1.5.md",sourceDirName:"docs/migration",slug:"/docs/migration/UPGRADE-1.5",permalink:"/docs/migration/UPGRADE-1.5",draft:!1,editUrl:"https://github.com/dbt-athena/dbt-athena.github.io/edit/main/docs/docs/migration/UPGRADE-1.5.md",tags:[],version:"current",lastUpdatedAt:1706174898,formattedLastUpdatedAt:"Jan 25, 2024",frontMatter:{title:"Upgrade to version 1.5",id:"UPGRADE-1.5"},sidebar:"docs",previous:{title:"Known issues",permalink:"/docs/known-issues"},next:{title:"Contributing",permalink:"/docs/contributing/"}},p={},c=[{value:"Highly available Hive table",id:"highly-available-hive-table",level:2},{value:"S3 naming strategies",id:"s3-naming-strategies",level:2},{value:"Hive snapshot migration",id:"hive-snapshot-migration",level:2}],m={toc:c},u="wrapper";function h(e){var t=e.components,a=(0,i.Z)(e,r);return(0,o.kt)(u,(0,n.Z)({},m,a,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h2",{id:"highly-available-hive-table"},"Highly available Hive table"),(0,o.kt)("p",null,"The 1.5 version introduces a breaking change. Materaliazation ",(0,o.kt)("inlineCode",{parentName:"p"},"table_hive_ha")," has been removed and integrated in the\n",(0,o.kt)("inlineCode",{parentName:"p"},"table")," materialization. To migrate your ",(0,o.kt)("inlineCode",{parentName:"p"},"table_hive_ha")," models change the configuration with the following properties:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"materialization=table")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"ha=true"))),(0,o.kt)("p",null,"More information ",(0,o.kt)("a",{parentName:"p",href:"/docs/configuration/materializations/hive"},"here")),(0,o.kt)("h2",{id:"s3-naming-strategies"},"S3 naming strategies"),(0,o.kt)("p",null,"The 1.5 version introduces a breaking change. If you are using ",(0,o.kt)("inlineCode",{parentName:"p"},"uuid")," strategy, it is now named ",(0,o.kt)("inlineCode",{parentName:"p"},"unique"),", so change\nyour config accordingly in your profiles file and your model & project YAML configs."),(0,o.kt)("h2",{id:"hive-snapshot-migration"},"Hive snapshot migration"),(0,o.kt)("p",null,"The hive snapshots in dbt-athena v1.4 diverged from the dbt-core implementation.\nIn the v1.5 release, we take the opportunity to review our approach so that we follow the dbt standards.\nUnfortunately, this introduces breaking changes to existing snapshots.\nWe did our best to mitigate the impact on the developer, by printing a detailed error message with migration instructions.\nThe easiest way to migrate your snapshot table is by running ",(0,o.kt)("inlineCode",{parentName:"p"},"dbt snapshot")," one snapshot at a time."),(0,o.kt)("p",null,"Below is a summary of the steps to migrate all your snapshots:"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"Install ",(0,o.kt)("inlineCode",{parentName:"li"},"dbt-athena>=1.5")),(0,o.kt)("li",{parentName:"ol"},"Choose a snapshot to migrate. It's probably best to start with one of the least important ones first."),(0,o.kt)("li",{parentName:"ol"},"Run ",(0,o.kt)("inlineCode",{parentName:"li"},"dbt snapshot SNAPSHOT_NAME"),". This command should fail, and print a large error message with a migration query. Scroll up the stdout output until you find the same query again, but without the log timestamps."),(0,o.kt)("li",{parentName:"ol"},"Go to Athena and run the queries one by one. Please double-check each query to make sure that it does what it needs to do. The queries perform the following steps:",(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},"Take a backup of the original table. You never know."),(0,o.kt)("li",{parentName:"ul"},"Run the necessary transformations for the migration and store the results in a temporary table."),(0,o.kt)("li",{parentName:"ul"},"Drop the original table."),(0,o.kt)("li",{parentName:"ul"},"Copy the temporary table results to the original table."),(0,o.kt)("li",{parentName:"ul"},"Drop the temporary table."))),(0,o.kt)("li",{parentName:"ol"},"Check the results of the migrated table. Run some sanity checks to see if it behaves correctly."),(0,o.kt)("li",{parentName:"ol"},"(Optional) Run ",(0,o.kt)("inlineCode",{parentName:"li"},"dbt snapshot SNAPSHOT_NAME")," again. This time there should be no error anymore."),(0,o.kt)("li",{parentName:"ol"},"Repeat steps 2-6 for all your snapshots."),(0,o.kt)("li",{parentName:"ol"},"(Optional) Run ",(0,o.kt)("inlineCode",{parentName:"li"},"dbt snapshot")," to make sure that all your snapshots are migrated."),(0,o.kt)("li",{parentName:"ol"},"(Optional) After a couple of dbt runs, you can clean up the backup tables.")))}h.isMDXComponent=!0},9252:(e,t,a)=>{"use strict";a.r(t),a.d(t,{assets:()=>p,contentTitle:()=>l,default:()=>h,frontMatter:()=>s,metadata:()=>d,toc:()=>c});var n=a(7462),i=a(3366),o=(a(7294),a(3905)),r=["components"],s={title:"How many dbt threads should I configure",id:"dbt-threads"},l=void 0,d={unversionedId:"faqs/Athena/dbt-threads",id:"faqs/Athena/dbt-threads",title:"How many dbt threads should I configure",description:"Athena has certain service API call quotas. The StartQueryExecution API supports 20 calls per second, and \u2013 if no api call is made in 4s \u2013 a burst to 80 calls. See the AWS Athena service limits documentation.",source:"@site/docs/faqs/Athena/dbt-threads.md",sourceDirName:"faqs/Athena",slug:"/faqs/Athena/dbt-threads",permalink:"/faqs/Athena/dbt-threads",draft:!1,editUrl:"https://github.com/dbt-athena/dbt-athena.github.io/edit/main/docs/faqs/Athena/dbt-threads.md",tags:[],version:"current",lastUpdatedAt:1706174898,formattedLastUpdatedAt:"Jan 25, 2024",frontMatter:{title:"How many dbt threads should I configure",id:"dbt-threads"},sidebar:"docs",previous:{title:"Frequently asked questions",permalink:"/docs/faqs"},next:{title:"Athena limits Hive tables to 100 partitions",permalink:"/faqs/Athena/too-many-open-partitions"}},p={},c=[],m={toc:c},u="wrapper";function h(e){var t=e.components,a=(0,i.Z)(e,r);return(0,o.kt)(u,(0,n.Z)({},m,a,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("p",null,"Athena has certain service API call quotas. The ",(0,o.kt)("inlineCode",{parentName:"p"},"StartQueryExecution")," API supports 20 calls per second, and \u2013 if no api call is made in 4s \u2013 a burst to 80 calls. See the ",(0,o.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/athena/latest/ug/service-limits.html#service-limits-api-calls"},"AWS Athena service limits documentation"),"."),(0,o.kt)("p",null,"Due to these limits, you should consider how many concurrent API requests are happening in your AWS account and configure your threads accordingly. A good average value for threads could be ",(0,o.kt)("inlineCode",{parentName:"p"},"threads: 8"),"."))}h.isMDXComponent=!0},9109:(e,t,a)=>{"use strict";a.r(t),a.d(t,{assets:()=>p,contentTitle:()=>l,default:()=>h,frontMatter:()=>s,metadata:()=>d,toc:()=>c});var n=a(7462),i=a(3366),o=(a(7294),a(3905)),r=["components"],s={title:"Athena limits Hive table to 100 partitions",sidebar_label:"Athena limits Hive tables to 100 partitions",id:"too-many-open-partitions"},l=void 0,d={unversionedId:"faqs/Athena/too-many-open-partitions",id:"faqs/Athena/too-many-open-partitions",title:"Athena limits Hive table to 100 partitions",description:"In some cases, you might experience HIVETOOMANYOPENPARTITIONS: Exceeded limit of 100 open writers for partitions/buckets.. Athena supports writing to 100 unique partition and bucket combinations per query. For example, if no buckets are defined in the destination table, you can specify a maximum of 100 partitions. If you specify five buckets, 20 partitions (each with five buckets) are allowed. If you exceed this count, an error occurs.",source:"@site/docs/faqs/Athena/too-many-open-partitions.md",sourceDirName:"faqs/Athena",slug:"/faqs/Athena/too-many-open-partitions",permalink:"/faqs/Athena/too-many-open-partitions",draft:!1,editUrl:"https://github.com/dbt-athena/dbt-athena.github.io/edit/main/docs/faqs/Athena/too-many-open-partitions.md",tags:[],version:"current",lastUpdatedAt:1706174898,formattedLastUpdatedAt:"Jan 25, 2024",frontMatter:{title:"Athena limits Hive table to 100 partitions",sidebar_label:"Athena limits Hive tables to 100 partitions",id:"too-many-open-partitions"},sidebar:"docs",previous:{title:"How many dbt threads should I configure",permalink:"/faqs/Athena/dbt-threads"}},p={},c=[],m={toc:c},u="wrapper";function h(e){var t=e.components,a=(0,i.Z)(e,r);return(0,o.kt)(u,(0,n.Z)({},m,a,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("p",null,"In some cases, you might experience ",(0,o.kt)("inlineCode",{parentName:"p"},"HIVE_TOO_MANY_OPEN_PARTITIONS: Exceeded limit of 100 open writers for partitions/buckets."),". Athena supports writing to 100 unique partition and bucket combinations per query. For example, if no buckets are defined in the destination table, you can specify a maximum of 100 partitions. If you specify five buckets, 20 partitions (each with five buckets) are allowed. If you exceed this count, an error occurs."),(0,o.kt)("admonition",{type:"info"},(0,o.kt)("p",{parentName:"admonition"},"Iceberg tables are also affected by the same limitation. The ",(0,o.kt)("inlineCode",{parentName:"p"},"iceberg.max-partitions-per-writer")," setting in Trino is set by default to 100.")),(0,o.kt)("p",null,"AWS suggests a workaround in the documentation using ",(0,o.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/athena/latest/ug/ctas-insert-into.html"},"CTAs and INSERT INTO"),". We can automate this in dbt using a materialization. An example implementation \u2013 inspired by the ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/dbt-labs/dbt-labs-experimental-features/tree/main/insert_by_period"},"Redshift insert_by_period materialization")," \u2013 can be found ",(0,o.kt)("a",{parentName:"p",href:"https://gist.github.com/jessedobbelaere/6fdb593f9e2cc732e9f142c56c9bac87"},"here"),"."))}h.isMDXComponent=!0}}]);